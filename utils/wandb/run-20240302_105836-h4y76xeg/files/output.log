/home/haoran/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
30it [00:00, 32.38it/s]
[34m[1mwandb[39m[22m: Ctrl + C detected. Stopping sweep.
Traceback (most recent call last):
  File "train_classification.py", line 187, in train
    classifier.cuda()
  File "train_classification.py", line 207, in train_epo
    loss += feature_transform_regularizer(trans_feat) * 0.001
  File "/home/haoran/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/haoran/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/haoran/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/haoran/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/home/haoran/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/home/haoran/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 351, in _single_tensor_adam
    step_t += 1
Exception